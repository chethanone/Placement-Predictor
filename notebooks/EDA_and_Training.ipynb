{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b61ecb67",
   "metadata": {},
   "source": [
    "# Placement Predictor - Complete Data Analysis & Model Training\n",
    "\n",
    "This notebook provides a comprehensive analysis and model training pipeline for predicting student placements.\n",
    "\n",
    "**Dataset Location:** Place your CSV file in `data/raw/placement_data.csv`\n",
    "\n",
    "## Notebook Structure:\n",
    "1. Import Libraries\n",
    "2. Load and Explore Dataset\n",
    "3. Data Cleaning and Handling Missing Values\n",
    "4. Exploratory Data Analysis (EDA)\n",
    "5. Feature Engineering\n",
    "6. Encode Categorical Variables\n",
    "7. Feature Scaling\n",
    "8. Train-Test Split\n",
    "9. Model Training\n",
    "10. Model Evaluation\n",
    "11. Hyperparameter Tuning\n",
    "12. Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba45139",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87851bef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Visualization libraries\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Machine learning libraries\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, cross_val_score, GridSearchCV\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ML Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report,\n",
    "                             roc_auc_score, roc_curve)\n",
    "\n",
    "# Model persistence\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db09e667",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset\n",
    "\n",
    "Load the placement dataset and examine its structure, dimensions, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc15f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Make sure to place your CSV file at: data/raw/placement_data.csv\n",
    "df = pd.read_csv('../data/raw/placement_data.csv')\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Number of Rows: {df.shape[0]}\")\n",
    "print(f\"Number of Columns: {df.shape[1]}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(\"=\"*60)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70c708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Information\n",
    "print(\"=\"*60)\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*60)\n",
    "df.info()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"=\"*60)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b09e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names and data types\n",
    "print(\"Column Names and Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Unique values per column:\")\n",
    "print(\"=\"*60)\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8c17c6",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Handling Missing Values\n",
    "\n",
    "Check for missing values, duplicates, and handle them appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d18e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=\"*60)\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(\"=\"*60)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "if missing.sum() == 0:\n",
    "    print(\"\\n‚úì No missing values found!\")\n",
    "else:\n",
    "    print(f\"\\nTotal missing values: {missing.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "plt.title('Missing Values Heatmap', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Columns')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db360ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"‚úì Removed {duplicates} duplicate rows\")\n",
    "    print(f\"New dataset shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"‚úì No duplicate rows found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c576ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (if any)\n",
    "# For numeric columns: fill with mean or median\n",
    "# For categorical columns: fill with mode\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "if df[numeric_cols].isnull().sum().sum() > 0:\n",
    "    imputer_numeric = SimpleImputer(strategy='mean')\n",
    "    df[numeric_cols] = imputer_numeric.fit_transform(df[numeric_cols])\n",
    "    print(\"‚úì Numeric missing values filled with mean\")\n",
    "\n",
    "if df[categorical_cols].isnull().sum().sum() > 0:\n",
    "    imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "    df[categorical_cols] = imputer_categorical.fit_transform(df[categorical_cols])\n",
    "    print(\"‚úì Categorical missing values filled with mode\")\n",
    "\n",
    "print(\"\\n‚úì Data cleaning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729b514",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Visualize the data to understand distributions, relationships, and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc1eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable Distribution\n",
    "# Adjust 'status' to your actual target column name\n",
    "target_col = df.columns[-1]  # Assuming last column is target\n",
    "print(f\"Target Column: {target_col}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df[target_col].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title(f'Distribution of {target_col}', fontsize=14, fontweight='bold')\n",
    "plt.xlabel(target_col)\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df[target_col].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=90)\n",
    "plt.title(f'{target_col} Percentage', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{target_col} Distribution:\")\n",
    "print(df[target_col].value_counts())\n",
    "print(f\"\\nPercentage:\")\n",
    "print((df[target_col].value_counts() / len(df) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f04be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric Features Distribution\n",
    "numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "if len(numeric_features) > 0:\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numeric_features) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(numeric_features):\n",
    "        if idx < len(axes):\n",
    "            axes[idx].hist(df[col].dropna(), bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "            axes[idx].set_title(f'Distribution of {col}', fontweight='bold')\n",
    "            axes[idx].set_xlabel(col)\n",
    "            axes[idx].set_ylabel('Frequency')\n",
    "            axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(numeric_features), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numeric features found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Features Distribution\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if target_col in categorical_features:\n",
    "    categorical_features.remove(target_col)\n",
    "\n",
    "if len(categorical_features) > 0:\n",
    "    n_cols = 2\n",
    "    n_rows = (len(categorical_features) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows * 4))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(categorical_features):\n",
    "        if idx < len(axes):\n",
    "            df[col].value_counts().plot(kind='bar', ax=axes[idx], color='coral')\n",
    "            axes[idx].set_title(f'Distribution of {col}', fontweight='bold')\n",
    "            axes[idx].set_xlabel(col)\n",
    "            axes[idx].set_ylabel('Count')\n",
    "            axes[idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(categorical_features), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No categorical features found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap\n",
    "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "if numeric_df.shape[1] > 1:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    correlation = numeric_df.corr()\n",
    "    sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=1)\n",
    "    plt.title('Correlation Heatmap', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated features\n",
    "    print(\"\\nHighly Correlated Feature Pairs (|correlation| > 0.7):\")\n",
    "    high_corr = []\n",
    "    for i in range(len(correlation.columns)):\n",
    "        for j in range(i+1, len(correlation.columns)):\n",
    "            if abs(correlation.iloc[i, j]) > 0.7:\n",
    "                high_corr.append((correlation.columns[i], correlation.columns[j], correlation.iloc[i, j]))\n",
    "    \n",
    "    if high_corr:\n",
    "        for feat1, feat2, corr in high_corr:\n",
    "            print(f\"{feat1} <-> {feat2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"No highly correlated features found\")\n",
    "else:\n",
    "    print(\"Not enough numeric features for correlation analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb20e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection\n",
    "if len(numeric_features) > 0:\n",
    "    fig, axes = plt.subplots(1, min(3, len(numeric_features)), figsize=(15, 5))\n",
    "    if len(numeric_features) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, col in enumerate(numeric_features[:3]):\n",
    "        sns.boxplot(y=df[col], ax=axes[idx], color='lightgreen')\n",
    "        axes[idx].set_title(f'Box Plot: {col}', fontweight='bold')\n",
    "        axes[idx].set_ylabel(col)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Outlier statistics\n",
    "    print(\"\\nOutlier Analysis (using IQR method):\")\n",
    "    for col in numeric_features:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "        if outliers > 0:\n",
    "            print(f\"{col}: {outliers} outliers ({outliers/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8be448",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "Create new features to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4882d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy for feature engineering\n",
    "df_fe = df.copy()\n",
    "\n",
    "# Example: Create average score features if you have percentage/score columns\n",
    "score_cols = [col for col in df_fe.columns if any(\n",
    "    keyword in col.lower() for keyword in ['percentage', 'cgpa', 'score', 'marks', '_p']\n",
    ")]\n",
    "\n",
    "if len(score_cols) >= 2:\n",
    "    df_fe['avg_academic_score'] = df_fe[score_cols].mean(axis=1)\n",
    "    df_fe['academic_consistency'] = df_fe[score_cols].std(axis=1)\n",
    "    print(f\"‚úì Created academic features from: {score_cols}\")\n",
    "    print(f\"  - avg_academic_score: average of all scores\")\n",
    "    print(f\"  - academic_consistency: standard deviation of scores\")\n",
    "else:\n",
    "    print(\"Not enough score columns found for feature engineering\")\n",
    "\n",
    "print(f\"\\nNew dataset shape: {df_fe.shape}\")\n",
    "df_fe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda8449",
   "metadata": {},
   "source": [
    "## 6. Encode Categorical Variables\n",
    "\n",
    "Convert categorical features to numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c89cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for categorical variables\n",
    "label_encoders = {}\n",
    "categorical_cols = df_fe.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"Encoding categorical variables:\")\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_fe[col] = le.fit_transform(df_fe[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"‚úì {col}: {len(le.classes_)} unique values -> {list(le.classes_)[:5]}\")\n",
    "\n",
    "print(f\"\\n‚úì All categorical variables encoded!\")\n",
    "print(f\"Final dataset shape: {df_fe.shape}\")\n",
    "df_fe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c77d0",
   "metadata": {},
   "source": [
    "## 7. Feature Scaling and Normalization\n",
    "\n",
    "Normalize features to ensure equal contribution to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_fe.drop(columns=[target_col])\n",
    "y = df_fe[target_col]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Save column names before scaling\n",
    "feature_names = X.columns.tolist()\n",
    "print(f\"\\nFeatures to be scaled: {len(feature_names)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5bb170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "\n",
    "print(\"‚úì Features scaled using StandardScaler\")\n",
    "print(f\"\\nScaled features - First 5 rows:\")\n",
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5ec89",
   "metadata": {},
   "source": [
    "## 8. Split Data into Training and Testing Sets\n",
    "\n",
    "Split the data for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f2b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"\\nFeatures: {X_train.shape[1]}\")\n",
    "print(f\"\\nTraining set target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTesting set target distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad4c7b",
   "metadata": {},
   "source": [
    "## 9. Model Selection and Training\n",
    "\n",
    "Train multiple classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c44ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "print(f\"‚úì Initialized {len(models)} models:\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"  - {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f3cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models and collect results\n",
    "results = {}\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Train Accuracy': train_acc,\n",
    "        'Test Accuracy': test_acc,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'CV Mean': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"  ‚úì Test Accuracy: {test_acc:.4f}, F1: {f1:.4f}, CV: {cv_scores.mean():.4f}\\n\")\n",
    "\n",
    "print(\"‚úÖ All models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a2f95",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation and Performance Metrics\n",
    "\n",
    "Compare model performance and select the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2340321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = results_df.index[0]\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"   Test Accuracy: {results_df.loc[best_model_name, 'Test Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Test Accuracy comparison\n",
    "results_df['Test Accuracy'].plot(kind='barh', ax=axes[0], color='skyblue')\n",
    "axes[0].set_xlabel('Test Accuracy', fontsize=12)\n",
    "axes[0].set_title('Model Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# F1 Score comparison\n",
    "results_df['F1 Score'].plot(kind='barh', ax=axes[1], color='lightcoral')\n",
    "axes[1].set_xlabel('F1 Score', fontsize=12)\n",
    "axes[1].set_title('Model F1 Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ad9cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation of best model\n",
    "best_model = models[best_model_name]\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"DETAILED EVALUATION: {best_model_name}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a93c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"TOP 10 IMPORTANT FEATURES ({best_model_name})\")\n",
    "    print(\"=\"*70)\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features = feature_importance.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'], color='teal')\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.title('Top 10 Feature Importance', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"\\n‚ö† {best_model_name} does not provide feature importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d192a3",
   "metadata": {},
   "source": [
    "## 11. Hyperparameter Tuning\n",
    "\n",
    "Fine-tune the best model using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a974fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for the best model\n",
    "# Define parameter grids for different models\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'min_samples_split': [2, 5]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'kernel': ['rbf', 'poly']\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs', 'saga']\n",
    "    }\n",
    "}\n",
    "\n",
    "if best_model_name in param_grids:\n",
    "    print(f\"üîß Hyperparameter tuning for {best_model_name}...\")\n",
    "    print(f\"Parameter grid: {param_grids[best_model_name]}\\n\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        models[best_model_name], \n",
    "        param_grids[best_model_name], \n",
    "        cv=5, \n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\n‚úì Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"‚úì Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    best_tuned_model = grid_search.best_estimator_\n",
    "    y_pred_tuned = best_tuned_model.predict(X_test)\n",
    "    test_acc_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "    \n",
    "    print(f\"‚úì Test accuracy (tuned): {test_acc_tuned:.4f}\")\n",
    "    print(f\"‚úì Improvement: {(test_acc_tuned - results_df.loc[best_model_name, 'Test Accuracy']):.4f}\")\n",
    "    \n",
    "    # Update best model\n",
    "    models[best_model_name] = best_tuned_model\n",
    "    best_model = best_tuned_model\n",
    "else:\n",
    "    print(f\"‚ö† Hyperparameter tuning not configured for {best_model_name}\")\n",
    "    print(\"Using the default model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9835d1",
   "metadata": {},
   "source": [
    "## 12. Save the Trained Model\n",
    "\n",
    "Save the best model and preprocessing objects for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "model_path = '../models/best_model.pkl'\n",
    "scaler_path = '../models/scaler.pkl'\n",
    "encoder_path = '../models/label_encoders.pkl'\n",
    "\n",
    "# Save model\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"‚úì Model saved to: {model_path}\")\n",
    "\n",
    "# Save scaler\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"‚úì Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save label encoders\n",
    "with open(encoder_path, 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "print(f\"‚úì Label encoders saved to: {encoder_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'test_accuracy': float(results_df.loc[best_model_name, 'Test Accuracy']),\n",
    "    'f1_score': float(results_df.loc[best_model_name, 'F1 Score']),\n",
    "    'feature_columns': feature_names,\n",
    "    'target_column': target_col\n",
    "}\n",
    "\n",
    "import json\n",
    "metadata_path = '../models/best_model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "print(f\"‚úì Metadata saved to: {metadata_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ All artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a134dc1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "- **Dataset**: [Your dataset size and features]\n",
    "- **Best Model**: [Model name with accuracy]\n",
    "- **Important Features**: [Top features that influence placement]\n",
    "\n",
    "### Next Steps:\n",
    "1. Use the saved model for predictions\n",
    "2. Deploy the model as a web application\n",
    "3. Collect more data to improve accuracy\n",
    "4. Try ensemble methods or deep learning\n",
    "\n",
    "### Files Created:\n",
    "- `../models/best_model.pkl` - Trained model\n",
    "- `../models/scaler.pkl` - Feature scaler\n",
    "- `../models/label_encoders.pkl` - Categorical encoders\n",
    "- `../models/best_model_metadata.json` - Model metadata\n",
    "\n",
    "---\n",
    "**Note**: Remember to update the dataset path (`data/raw/placement_data.csv`) before running the notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
